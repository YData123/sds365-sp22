{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Machine Learning: Assignment 3\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "Assignment 3 is due Wednesday, April 13 by 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on Canvas).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "Submit your assignment as a pdf file on Gradescope, and as a notebook (.ipynb) on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:\n",
    "\n",
    "Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "Under \"Download as\", select \"HTML (.html)\"\n",
    "After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "\n",
    " * Variational autoencoders\n",
    " * Undirected graphs\n",
    " * The graphical lasso\n",
    "\n",
    "This assignment will also help to solidify your Python and Jupyter notebook skills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Face time (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we will implement a \"shoestring\" version of [this amazing fake face generator](https://www.nytimes.com/interactive/2020/11/21/science/artificial-intelligence-fake-people-faces.html), using a variational autoencoder (VAE). Building a generator like the one featured in the article can take a tremendous amount of computational resources, time, and parameter tuning. In this problem we will build a basic version to illustrate the main concepts, and help you to become more famililar with VAEs.  Here is an outline of the process that we'll step you through:\n",
    "\n",
    "### Problem outline:\n",
    "\n",
    "* Load data\n",
    "* Create face groups based on attributes\n",
    "* Construct the VAE\n",
    "* Define the loss function and train the VAE (Problem 1.1)\n",
    "* Encode and reconstruct faces (Problem 1.2)\n",
    "* Visualize the latent space (Problem 1.3)\n",
    "* Morph between faces (Problem 1.4)\n",
    "* Shift attributes of faces (Problem 1.5)\n",
    "* Generate new faces (Problem 1.6)\n",
    "* Analyze the effect of the scaling factor in the loss function (Problem 1.7, optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we load the packages that we'll need. If you don't have one or more of these, you can install them with `!pip install <package_name>` in the cell, or outside the notebook \n",
    "with `conda install -c conda-forge <package_name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "import tensorflow\n",
    "tensorflow.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) (LFW) is a database of face photographs. The images are placed in the folder lfw-deepfunneled. lfw_attributes.txt is a document including a set of attributes associated for each image, such as 'Male', 'Smile', 'Bold', etc. All the features are numerical and large positive values indicate that the keywords well describe the photo; large negative values indicate that the keywords don't fit the photo.\n",
    "\n",
    "For this problem, we will keep only the middle parts of the photos to avoid complex backgrounds.\n",
    "\n",
    "Download the data from the cloud at these URLs: \n",
    "\n",
    "https://sds365.s3.amazonaws.com/lfw/lfw-deepfunneled.zip\n",
    "<br>\n",
    "https://sds365.s3.amazonaws.com/lfw/lfw_attributes.txt \n",
    "\n",
    "Once you have the data, unzip it, and place it in a directory that we will call \"YOUR_PATH\" below.\n",
    "  \n",
    "Run all the cells in this section to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH =\"/YOUR_PATH/lfw-deepfunneled/\"\n",
    "ATTRIBUTES_PATH = \"/YOUR_PATH/lfw_attributes.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for path in glob.iglob(os.path.join(DATASET_PATH, \"**\", \"*.jpg\")):\n",
    "    person = path.split(\"/\")[-2]\n",
    "    dataset.append({\"person\":person, \"path\": path})\n",
    "    \n",
    "dataset = pd.DataFrame(dataset)\n",
    "dataset = dataset.groupby(\"person\").filter(lambda x: len(x) < 100 )\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will display some sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampled_id = []\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i in range(20):\n",
    "    idx = random.randint(0, len(dataset))\n",
    "    img = plt.imread(dataset.path.iloc[idx])\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(dataset.person.iloc[idx])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    sampled_id.append(idx)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the images with some of the background removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dx=70\n",
    "dy=70\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i in range(20):\n",
    "    idx = sampled_id[i]\n",
    "    img = plt.imread(dataset.path.iloc[idx])\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(img[dy:-dy,dx:-dx])\n",
    "    plt.title(dataset.person.iloc[idx])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fetch_dataset(dx=70,dy=70, dimx=45,dimy=45):\n",
    "    \n",
    "    df_attrs = pd.read_csv(ATTRIBUTES_PATH, sep='\\t', skiprows=1,) \n",
    "    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n",
    "    \n",
    "    photo_ids = []\n",
    "    for dirpath, dirnames, filenames in os.walk(DATASET_PATH):\n",
    "        for fname in filenames:\n",
    "            if fname.endswith(\".jpg\"):\n",
    "                fpath = os.path.join(dirpath,fname)\n",
    "                photo_id = fname[:-4].replace('_',' ').split()\n",
    "                person_id = ' '.join(photo_id[:-1])\n",
    "                photo_number = int(photo_id[-1])\n",
    "                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n",
    "\n",
    "    photo_ids = pd.DataFrame(photo_ids)\n",
    "    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n",
    "\n",
    "    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n",
    "\n",
    "    all_photos = df['photo_path'].apply(imageio.imread)\\\n",
    "                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n",
    "                                .apply(lambda img: np.array(Image.fromarray(img).resize([dimx,dimy])) )\n",
    "\n",
    "    all_photos = np.stack(all_photos.values).astype('uint8')\n",
    "    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n",
    "    \n",
    "    return all_photos,all_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `data` has all the face images and the variable `attrs` has all the attributes. The 8-bit RGB values are converted to values between 0 and 1 for modeling and plotting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, attrs = fetch_dataset()\n",
    "data = np.array(data / 255, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Face Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create groups of faces, by selecting the faces having the highest or lowest scores for each of the attributes. Run all the cells in this section to create and plot some face groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_gallery(images, h, w, n_row=3, n_col=6, with_title=False, titles=[]):\n",
    "    plt.figure(figsize=(1.75 * n_col, 2 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        try:\n",
    "            plt.imshow(images[i].reshape((h, w, 3)), cmap=plt.cm.gray, vmin=-1, vmax=1, interpolation='nearest')\n",
    "            if with_title:\n",
    "                plt.title(titles[i])\n",
    "            plt.xticks(())\n",
    "            plt.yticks(())\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IMAGE_H = data.shape[1]\n",
    "IMAGE_W = data.shape[2]\n",
    "N_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smile_ids = attrs['Smiling'].sort_values(ascending=False).head(36).index.values\n",
    "smile_data = data[smile_ids]\n",
    "\n",
    "no_smile_ids = attrs['Smiling'].sort_values(ascending=True).head(36).index.values\n",
    "no_smile_data = data[no_smile_ids]\n",
    "\n",
    "eyeglasses_ids = attrs['Eyeglasses'].sort_values(ascending=False).head(36).index.values\n",
    "eyeglasses_data = data[eyeglasses_ids]\n",
    "\n",
    "sunglasses_ids = attrs['Sunglasses'].sort_values(ascending=False).head(36).index.values\n",
    "sunglasses_data = data[sunglasses_ids]\n",
    "\n",
    "mustache_ids = attrs['Mustache'].sort_values(ascending=False).head(36).index.values\n",
    "mustache_data = data[mustache_ids]\n",
    "\n",
    "male_ids = attrs['Male'].sort_values(ascending=False).head(36).index.values\n",
    "male_data = data[male_ids]\n",
    "\n",
    "female_ids = attrs['Male'].sort_values(ascending=True).head(36).index.values\n",
    "female_data = data[female_ids]\n",
    "\n",
    "eyeclosed_ids = attrs['Eyes Open'].sort_values(ascending=True).head(36).index.values\n",
    "eyeclosed_data = data[eyeclosed_ids]\n",
    "\n",
    "mouthopen_ids = attrs['Mouth Wide Open'].sort_values(ascending=False).head(36).index.values\n",
    "mouthopen_data = data[mouthopen_ids]\n",
    "\n",
    "makeup_ids = attrs['Heavy Makeup'].sort_values(ascending=False).head(36).index.values\n",
    "makeup_data = data[makeup_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(smile_data, IMAGE_H, IMAGE_W, n_row=6, n_col=6, with_title=True, titles=smile_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(no_smile_data, IMAGE_H, IMAGE_W, n_row=6, n_col=6, with_title=True, titles=no_smile_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(eyeglasses_data, IMAGE_H, IMAGE_W, n_row=6, n_col=6, with_title=True, titles=eyeglasses_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(sunglasses_data, IMAGE_H, IMAGE_W, n_row=6, n_col=6, with_title=True, titles=sunglasses_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(mustache_data, IMAGE_H, IMAGE_W, n_row=6, n_col=6, with_title=True, titles=mustache_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(male_data, IMAGE_H, IMAGE_W, n_row=6, n_col=6, with_title=True, titles=male_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(female_data, IMAGE_H, IMAGE_W, n_row=6, n_col=6, with_title=True, titles=female_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(eyeclosed_data, IMAGE_H, IMAGE_W, n_row=6, n_col=6, with_title=True, titles=eyeclosed_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(mouthopen_data, IMAGE_H, IMAGE_W, n_row=6, n_col=6, with_title=True, titles=mouthopen_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_gallery(makeup_data, IMAGE_H, IMAGE_W, n_row=6, n_col=6, with_title=True, titles=makeup_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the encoder\n",
    "\n",
    "Recall that the encoder part of the VAE architecture maps a data point to a variational mean and (log) variance. The mean is a point in the latent space. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LATENT_SPACE_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"reparameterization trick\" draws samples from the variational distribution that are parameterized by the variational mean and variance, so that the parameters of the encoder network can be trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample_latent_features(distribution):\n",
    "    distribution_mean, distribution_variance = distribution\n",
    "    batch_size = tensorflow.shape(distribution_variance)[0]\n",
    "    random = tensorflow.keras.backend.random_normal(shape=(batch_size, tensorflow.shape(distribution_variance)[1]))\n",
    "    return distribution_mean + tensorflow.exp(0.5 * distribution_variance) * random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_data = tensorflow.keras.layers.Input(shape=(45, 45, 3))\n",
    "\n",
    "encoder = tensorflow.keras.layers.Conv2D(64, (5,5), activation='relu')(input_data)\n",
    "encoder = tensorflow.keras.layers.MaxPooling2D((2,2))(encoder)\n",
    "\n",
    "encoder = tensorflow.keras.layers.Conv2D(64, (3,3), activation='relu')(encoder)\n",
    "encoder = tensorflow.keras.layers.MaxPooling2D((2,2))(encoder)\n",
    "\n",
    "encoder = tensorflow.keras.layers.Conv2D(32, (3,3), activation='relu')(encoder)\n",
    "encoder = tensorflow.keras.layers.MaxPooling2D((2,2))(encoder)\n",
    "\n",
    "encoder = tensorflow.keras.layers.Flatten()(encoder)\n",
    "\n",
    "distribution_mean = tensorflow.keras.layers.Dense(LATENT_SPACE_SIZE, name='variational_mean')(encoder)\n",
    "distribution_variance = tensorflow.keras.layers.Dense(LATENT_SPACE_SIZE, name='variational_log_variance')(encoder)\n",
    "latent_encoding = tensorflow.keras.layers.Lambda(sample_latent_features)([distribution_mean, distribution_variance])\n",
    "\n",
    "encoder_model = tensorflow.keras.Model(input_data, latent_encoding)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the decoder\n",
    "\n",
    "The decoder network in the VAE architecture maps a latent vector to an image. \n",
    "This is done with a series of transposed convolutional layers, since it must \n",
    "map from low to high dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoder_input = tensorflow.keras.layers.Input(shape=LATENT_SPACE_SIZE)\n",
    "decoder = tensorflow.keras.layers.Dense(64)(decoder_input)\n",
    "decoder = tensorflow.keras.layers.Reshape((1, 1, 64))(decoder)\n",
    "decoder = tensorflow.keras.layers.Conv2DTranspose(64, (3,3), activation='relu')(decoder)\n",
    "decoder = tensorflow.keras.layers.UpSampling2D((2,2))(decoder)\n",
    "\n",
    "decoder = tensorflow.keras.layers.Conv2DTranspose(64, (3,3), activation='relu')(decoder)\n",
    "decoder = tensorflow.keras.layers.UpSampling2D((2,2))(decoder)\n",
    "\n",
    "decoder = tensorflow.keras.layers.Conv2DTranspose(64, (5,5), activation='relu')(decoder)\n",
    "decoder = tensorflow.keras.layers.UpSampling2D((2,2))(decoder)\n",
    "\n",
    "decoder_output = tensorflow.keras.layers.Conv2DTranspose(3, (6,6), activation='relu')(decoder)\n",
    "\n",
    "decoder_model = tensorflow.keras.Model(decoder_input, decoder_output)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded = encoder_model(input_data)\n",
    "decoded = decoder_model(encoded)\n",
    "autoencoder = tensorflow.keras.models.Model(input_data, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_loss(distribution_mean, distribution_variance, factor):\n",
    "    \n",
    "    def get_reconstruction_loss(y_true, y_pred, factor):\n",
    "        reconstruction_loss = tensorflow.keras.losses.mse(y_true, y_pred)\n",
    "        reconstruction_loss_batch = tensorflow.reduce_mean(reconstruction_loss)\n",
    "        return reconstruction_loss_batch*factor\n",
    "    \n",
    "    def get_kl_loss(distribution_mean, distribution_variance):\n",
    "        kl_loss = 1 + distribution_variance - tensorflow.square(distribution_mean) - tensorflow.exp(distribution_variance)\n",
    "        kl_loss_batch = tensorflow.reduce_mean(kl_loss)\n",
    "        return kl_loss_batch*(-0.5)\n",
    "    \n",
    "    def total_loss(y_true, y_pred):\n",
    "        reconstruction_loss_batch = get_reconstruction_loss(y_true, y_pred, factor)\n",
    "        kl_loss_batch = get_kl_loss(distribution_mean, distribution_variance)\n",
    "        return reconstruction_loss_batch + kl_loss_batch\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Deriving the loss function (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the loss function defined in the cell above from the probability model perspective. You can ignore the scalar `factor` in your derivation. Show your work using either LaTeX or a picture of your written solution. \n",
    "\n",
    "Hint: Think about how how the total loss is related to the ELBO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your derivation here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three cells train the model. You can just run them. It may take a while to run on your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(loss=get_loss(distribution_mean, distribution_variance, factor = 45*45*3), optimizer='adam')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(data, test_size=0.2, random_state=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=64, validation_data=(X_val, X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reconstructing faces (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell encodes and reconstruct 16 random faces from the validation set with the trained VAE. Run the cell and comment on the reconstrcted faces. (3 points)\n",
    "* Do the reconstructed faces resemble the original images? How are they similar/different?\n",
    "* Are there any faces that are reconstructed better or worse than the others? Can you think of why?\n",
    "* Comment on any other aspects of your findings that are interesting to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_index = random.sample(range(1, len(X_val)), 16)\n",
    "\n",
    "fig, axs = plt.subplots(4, 8)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axs[i, 2*j].imshow(X_val[sample_index[4*i+j], :, :, :])\n",
    "        axs[i, 2*j].axis('off')\n",
    "        axs[i, 2*j+1].imshow(np.clip(autoencoder.predict(np.array([X_val[sample_index[4*i+j], :, :, :]]))[0],0,1))\n",
    "        axs[i, 2*j+1].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visualizing the latent space (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `vae_demo` from class, the MNIST digits were generated from a two-dimensional latent space. In the current model, the latent space has more than two dimensions, so to visualize it we need to use a dimensionality reduction technique. (If you are not familiar with PCA, please refer to the material for Week 7 of [iML](https://ydata123.org/fa21/iml/calendar.html).)\n",
    "\n",
    "In this problem, you will first implement the function `LatentSpace_2D`. (6 points)\n",
    "1. Calculate the latent space encodings for two sets of faces that are different in one attribute, e.g. smile vs. no smile.\n",
    "2. Use PCA to reduce the dimension of the latent space codes to two.\n",
    "3. Visualize the latent space after dimensionality reduction with a scatter plot. Clearly color-code and label the two different groups.\n",
    "\n",
    "Here is an example using smile_data and no_smile_data.\n",
    "![Sample_1_3](https://raw.githubusercontent.com/YData123/sds365-sp22/main/assignments/assn3/Sample_1_3.png)\n",
    "\n",
    "Visualize the latent space for at least three pairs of face groups including smile vs. no smile. Comment on how the scatter plots look.\n",
    "* Are the two groups separable in the two-dimensional latent space? Is this what you expected? Why or why not? (2 points)\n",
    "* How do the plots for the three different attributes differ from each other? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LatentSpace_2D(encoder_model, data1, label1, data2, label2):\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LatentSpace_2D(encoder_model, smile_data, 'Smile', no_smile_data, 'No smile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LatentSpace_2D(encoder_model,smile_data,'No sunglasses',sunglasses_data,'Sunglasses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LatentSpace_2D(encoder_model,male_data,'Male',female_data,'Female')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Morphing between faces (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morph at least 5 pairs of faces with the function `morphBetweenImages` and comment on what you observe.\n",
    "* Briefly explain how the morphing works. (2 points)\n",
    "* Do the generated faces look like what you expected? Does any of the pairs work better than the others? If so, what kind of image pairs work better? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Don't change the function\n",
    "def morphBetweenImages(img1, img2, num_of_morphs):\n",
    "    alpha = np.linspace(0,1,num_of_morphs)\n",
    "    z1 = encoder_model.predict(np.array([img1]))\n",
    "    z2 = encoder_model.predict(np.array([img2]))\n",
    "    fig = plt.figure(figsize=(30,5))\n",
    "    \n",
    "    ax = fig.add_subplot(1, num_of_morphs+2, 1)\n",
    "    ax.imshow(img1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(loc='center', label='original image 1',fontsize=10)\n",
    "    \n",
    "    for i in range(num_of_morphs):\n",
    "        z = z1*(1-alpha[i]) + z2*alpha[i]\n",
    "        new_img = decoder_model.predict(z)\n",
    "        \n",
    "        ax = fig.add_subplot(1, num_of_morphs+2, i+2)\n",
    "        ax.imshow(np.clip(new_img.squeeze(),0,1))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(loc='center', label='alpha={:.2f}'.format(alpha[i]))\n",
    "        \n",
    "    ax = fig.add_subplot(1, num_of_morphs+2, num_of_morphs+2)\n",
    "    ax.imshow(img2)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(loc='center', label='original image 2',fontsize=10)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_index = random.sample(range(1, len(data)), 2)\n",
    "morphBetweenImages(data[sample_index[0]],data[sample_index[1]],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Attribute shift (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1.3, we've seen that faces with the same attributes form clusters in the latent space. In this problem, you will implement a function `AttributeShift` to change one attribute of the faces.\n",
    "\n",
    "First implement the function `AttributeShift`. (5 points)\n",
    "1. Calculate the latent space codes for two sets of faces that are different in one attribute, e.g. smile vs. no smile.\n",
    "2. Calculate the mean latent space code for each group.\n",
    "3. Get the attribute shifting vector by taking the difference between the two codes.\n",
    "4. Perform attribute shift by adding the attribute shifting vector to the latent space code of the faces you want to modify.\n",
    "5. Generate the image using the new latent space codes.\n",
    "\n",
    "Here is a diagram demonstrating the shift in the latent space. Please note that the two-dimensional latent space is just for demonstration purpose. You should *not* use PCA in this problem. Instead, use the original latent space.\n",
    "![Diagram_1_5](./Diagram_1_5.png)\n",
    "\n",
    "Perform attribute shift on at three attributes including smile. Comment on the faces with shifted attributes. (5 points)\n",
    "* Do the generated faces look like what you expected? If not, can you think of some possible reasons?\n",
    "* Do the faces with new attributes resemble the original faces? If not, can you think of some possible reasons?\n",
    "* Which of the attribute shift is more successful? What are some possible reasons?\n",
    "* Comment on any other aspects of your findings that are interesting to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Don't change this helper function!\n",
    "def PlotAttributeShift(data2,pic_output):\n",
    "    sample_index = random.sample(range(1, len(data2)), 16)\n",
    "    \n",
    "    fig, axs = plt.subplots(4, 8)\n",
    "    fig.set_figheight(10)\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axs[i, 2*j].imshow(data2[sample_index[4*i+j], :, :, :])\n",
    "            axs[i, 2*j].axis('off')\n",
    "            axs[i, 2*j+1].imshow(np.clip(pic_output[sample_index[4*i+j]],0,1))\n",
    "            axs[i, 2*j+1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def AttributeShift(encoder_model,decoder_model,data1,data2):\n",
    "    # Your code here\n",
    "    return pic_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pic_output = AttributeShift(encoder_model,decoder_model,smile_data,no_smile_data)\n",
    "PlotAttributeShift(no_smile_data,pic_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Generating new faces (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational autoencoders can be used to generate new data; this is why they are generative models. We can sample new data points from the distribution in latent space and reconstruct new, fake faces based on them.\n",
    "\n",
    "To draw a sample close to an existing sample in the latent space, we can add a scaled random sample from the normal distribution to the latent space code of an existing sample. The scalar, which we call the `noise_level` is a parameter that we can tune.\n",
    "\n",
    "Run `GenerateFaces` with three different values of `noise_level` and comment on the generated faces. (3 points)\n",
    "* Do the generated images look like faces?\n",
    "* What happens when the new samples diverge more from the existing samples? What is a possible reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def GenerateFaces(data, LATENT_SPACE_SIZE, noise_level):\n",
    "    sample_index = random.sample(range(1, len(data)), 15)\n",
    "    latent_space = noise_level*np.random.normal(size=(15,LATENT_SPACE_SIZE))+encoder_model.predict(data[sample_index])\n",
    "    generated_image = decoder_model.predict(latent_space)\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i in range(number_of_images):\n",
    "        ax = fig.add_subplot(3, 5, i+1)\n",
    "        ax.imshow(np.clip(generated_image[i, :,:,:],0,1))\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GenerateFaces(data,LATENT_SPACE_SIZE,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GenerateFaces(data,LATENT_SPACE_SIZE,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GenerateFaces(data,LATENT_SPACE_SIZE,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Changing the loss function (2 points extra credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `factor` is a parameter of the loss function. In this optional problem, you will play with it and think about its effect on the performance of VAE.\n",
    "\n",
    "Retrain the model with a smaller factor. Repeat 1.2, 1.6 (using `noise_level` = 1) and latent space visualization. Comment on how the reconstruction results, generated new faces and latent space distributions change. (2 points)\n",
    "* Which model reconstructs the faces better?\n",
    "* Do the generated faces look different?\n",
    "* How do the latent variable distributions differ?\n",
    "* Do the differences make sense? Can you explain what you observed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\reals}{{\\mathbb R}}\n",
    "\\newcommand{\\indp}{\\perp\\kern-4pt\\perp}\n",
    "\\newcommand{\\given}{\\,|\\,}\n",
    "$\n",
    "\n",
    "## Problem 2: Are you Schur? (10 points)\n",
    "\n",
    "The graphical lasso is based on conditional independence properties \n",
    "of Gaussian distributions. This problem asks you to reason about the graphs\n",
    "underlying a multivariate Gaussian.\n",
    "\n",
    "Let $X = (Y,Z) \\in \\reals^6 \\sim N(0, \\Sigma)$ be a random\n",
    "Gaussian vector where $Y = (Y_1,Y_2) \\in\\reals^2$ and\n",
    "$Z = (Z_1,Z_2,Z_3,Z_4) \\in\\reals^4$, with\n",
    "$\\Sigma^{-1} = \\Omega =  \\begin{pmatrix}\n",
    "A & B \\\\\n",
    "B^T & C\n",
    "\\end{pmatrix}\n",
    "$\n",
    "where\n",
    "$$\n",
    "A = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2\\end{pmatrix} \\qquad\n",
    "B = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{3} & \\frac{1}{4} \\\\[1pt]\n",
    "                     -1 & \\frac{1}{2} & -\\frac{1}{3} & \\frac{1}{4}\n",
    "\\end{pmatrix} \\qquad\n",
    "C = \\begin{pmatrix}\n",
    "2 & \\frac{1}{2} & 0 & 0 \\\\\n",
    "\\frac{1}{2} & 2 & 0 & 0 \\\\\n",
    "0 & 0 & 2 & \\frac{1}{2} \\\\\n",
    "0 & 0 & \\frac{1}{2} & 2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "1. Draw the undirected graph of $X$, arranging the vertices in a hexagon. Explain your answer.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Draw the undirected graph of $Z$, arranging the vertices in a square.  Explain your answer.\n",
    "Hint:  The \n",
    "[Schur complement](https://en.wikipedia.org/wiki/Schur_complement) \n",
    "$ S= C  - B^T A^{-1} B$ has the property that \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{pmatrix}\n",
    "A & B \\\\\n",
    "B^T & C\n",
    "\\end{pmatrix}^{-1}\n",
    "\\;=\\;\n",
    "\\begin{pmatrix}\n",
    "A^{-1} + A^{-1} B S^{-1} B ^T A^{-1} & - A^{-1} B S^{-1}  \\\\\n",
    "- S^{-1} B^T A^{-1} & S^{-1}\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Which of the following conditional independence statements hold? Explain your answers.\n",
    "\n",
    "* $Y_1 \\indp Y_2 \\given Z$\n",
    "* $Z_1 \\indp Z_4 \\given Z_2$\n",
    "* $Z_1 \\indp Z_4 \\given Y_1$\n",
    "* $Z_1 \\indp Z_2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Taking stock (10 points)\n",
    "\n",
    "A joint distribution of data has a natural graph associated with it. When the distribution is multivariate normal, this graph is encoded in the pattern of zeros and non-zeros in the inverse of the covariance matrix, also known as the \"precision matrix.\"\n",
    "\n",
    "In class we demonstrated the graphical lasso for estimating the graph on ETF data.\n",
    "In this problem you will construct two different \"portolios\" of stocks, \n",
    "and run the graphical lasso to estimate a graph, commenting on your results.\n",
    "\n",
    "All of the code you might need for this is contained in the demo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading data\n",
    "\n",
    "As demonstrated in class, you will run on equities data downloaded from Yahoo finance.\n",
    "Your job is to construct two \"portfolios\" of stocks, each of which has some kind of organization to it. For example, in one portfolio you might have 5 energy stocks, 5 tech stocks, 5 consumer staples stocks, and 5 ETF stocks. Each portfolio should have at least 20 stocks. \n",
    "\n",
    "To download the data, follow the procedure outlined below (and discussed briefly in class):\n",
    "\n",
    "  * Search on a ticker symbol, like EZA, using [https://finance.yahoo.com/quote/EZA/history?p=EZA](https://finance.yahoo.com/quote/EZA/history?p=EZA)\n",
    "  * Select the range of the query, the frequency (daily, weekly, or monthly) and then issue the query. This will give you results like this:\n",
    "  \n",
    " ![ezh](https://raw.githubusercontent.com/YData123/sds365-sp22/main/demos/graphs/ezh.png)\n",
    " \n",
    "  * Next, hover over the Download link, and grab the URL. In this case it gives \n",
    "  https://query1.finance.yahoo.com/v7/finance/download/EZA?period1=1044576000&period2=1648080000&interval=1d&events=history&includeAdjustedClose=true\n",
    "  \n",
    "  * Then, you can use this same URL, but swap in different ticker symbols, to get the corresponding data for range of companies or funds.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing  your portfolios\n",
    "\n",
    "Your task is to analyze each porfolio using the graphical lasso, and comment on your findings.\n",
    "Here are the types of questions you should address:\n",
    "\n",
    "* How did you choose the portolio? How did you choose the date range and frequency (daily, weekly, etc.)? Remember, each of the portfolios must contain at least 20 stocks, and be organized in some reasonable way.\n",
    "\n",
    "* Display the graph obtained with the graphical lasso, using networkx. How did you choose the regularization level? Does the structure of the graph make sense? Is it sensitive to the choice of regularization level? Is this the structure you expected to see when you designed the portfolio? Why or why not?\n",
    "\n",
    "* What are some of the conditional independence assumptions implied by the graph? Are some parts of the graph more densely connected than others? Why?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
