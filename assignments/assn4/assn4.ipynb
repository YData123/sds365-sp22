{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Machine Learning: Assignment 4\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "Assignment 4 is due Wednesday, April 27 by 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on Canvas).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on Canvas. You can also post questions or start discussions on Ed Discussion. The assignment may look long at first glance, but the problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission**\n",
    "\n",
    "Submit your assignment as a pdf file on Gradescope, and as a notebook (.ipynb) on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to more easily find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:\n",
    "\n",
    "Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "Under \"Download as\", select \"HTML (.html)\"\n",
    "After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "\n",
    " * Graph kernels\n",
    " * Reinforcement learning\n",
    " * Recurrent neural networks\n",
    "\n",
    "This assignment will also help to solidify your Python skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\reals}{{\\mathbb R}}\n",
    "\\newcommand{\\indp}{\\perp\\kern-4pt\\perp}\n",
    "\\newcommand{\\given}{\\,|\\,}\n",
    "$\n",
    "\n",
    "## Problem 1: Graph kernels (10 points)\n",
    "\n",
    "The graph Laplacian for a weighted graph on $n$ nodes is defined as\n",
    "\n",
    "$$ L = D - W$$\n",
    "\n",
    "where $W$ is an $n\\times n$ symmetric matrix of positive edge weights,\n",
    "with $W_{ij} = 0$ if $(i,j)$ is not an edge in the graph,\n",
    "and $D$ is the diagonal matrix with $D_{ii} = \\sum_{j=1}^n W_{ij}$.\n",
    "This generalizes the definition of the Laplacian\n",
    "used in class, where all of the edge weights are one.\n",
    "\n",
    "\n",
    "1. Show that $L$ is a Mercer kernel, by showing that $L$ is\n",
    "  symmetric and positive-semidefinite.\n",
    "<br>\n",
    "\n",
    "2. In graph neural networks we define polynomial filters of the form\n",
    "\n",
    "  $$ P = a_0 I + a_1 L + a_2 L^2 + \\cdots a_d L^d$$\n",
    "  \n",
    "  where $L$ is the Laplacian and $a_0,\\ldots, a_d$ are parameters,\n",
    "  corresponding to the filter parameters in standard convolutional\n",
    "  neural networks.\n",
    "\n",
    "  If each $a_i \\geq 0$ is non-negative, show that $P$ is also\n",
    "  a Mercer kernel. \n",
    "<br>\n",
    "  \n",
    "3. Is positivity of the coefficients $a_i$ a necessary condition? Explain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Problem 2:  Positive reinforcement  (10 points)\n",
    "$\\def\\J{{\\mathcal J}}$\n",
    "$\\def\\E{{\\mathbb E}}$\n",
    "\n",
    "As discussed in class, reinforcement learning\n",
    "using policy gradient methods is based on maximizing the\n",
    "expected total reward\n",
    "\n",
    "$$ \\J(\\theta) = \\E_\\theta [R(\\tau)],$$\n",
    "\n",
    "where the expectation is over the probability distribution over sequences $\\tau$ through a choice of actions using the policy. This can be rewritten as\n",
    "\n",
    "\\begin{align*}\n",
    "  \\nabla_\\theta \\J(\\theta) & = \\E_\\theta\\left[ R(\\tau) \\nabla_\\theta \\log p(\\tau\\given \\theta) \\right].\n",
    "\\end{align*}\n",
    "\n",
    "Approximating this gradient involves computing $\\nabla_\\theta \\log \\pi_\\theta (a\\given s)$ where $\\pi_\\theta$ is the policy.\n",
    "\n",
    "Suppose that the action space is continuous\n",
    "and $\\pi_\\theta(a\\given s)$ is a normal density with mean\n",
    "$\\mu_\\theta(s)$ and variance $\\sigma^2_\\theta(s)$, two outputs of\n",
    "a neural network with input $s$ and parameters $\\theta$.\n",
    "\n",
    "1. Suppose the outputs of the neural network are given by\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mu_\\theta(s) & = \\beta_1^T h(s) \\\\\n",
    "  \\sigma^2_\\theta(s) &= \\mbox{sigmoid}(\\beta_2^T h(s))\n",
    "\\end{align*}\n",
    "\n",
    "where $h(s)$ is the vector of neurons in the last layer, immediately\n",
    "before the outputs. Derive explicit expressions for\n",
    "$\\nabla_{\\beta_1} \\log \\pi_\\theta(a\\given s)$ and\n",
    "$\\nabla_{\\beta_2} \\log \\pi_\\theta(a\\given s)$.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Explain how these gradients and other gradient\n",
    "terms in $\\nabla_\\theta \\log \\pi_\\theta(a\\given s)$ are used\n",
    "to estimate the policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Deep Q-Learning for Flappy Bird (25 points)\n",
    "\n",
    "Deep Q-learning was proposed (and patented) by DeepMind and made \n",
    "a big splash when the same deep neural network architecture was shown to be able to surpass\n",
    "human performance on many different Atari games, playing directly from the pixels.\n",
    "In this problem, we will walk you through the implementation of deep Q-learning \n",
    "to learn to play the Flappy Bird game.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/YData123/sds365-sp22/main/assignments/assn4/images/flappy_bird_demp.gif\" width=\"144\" height=\"256\"/>\n",
    "\n",
    "The implementation is based these references:\n",
    "- [DeepLearningFlappyBird](https://github.com/yenchenlin/DeepLearningFlappyBird)\n",
    "- [Deep Q-Learning for Atari Breakout](https://keras.io/examples/rl/deep_q_network_breakout/)\n",
    "\n",
    "We use the `pygame` package to visualize the interaction between the algorithm \n",
    "and the game environment. \n",
    "However, _pygame_ is not well supported by Google Colab; \n",
    "we recommend you to run the code for this problem locally.\n",
    "A window will be popped up that displays\n",
    "the game as it progress in real-time (as for the Cartpole demo from class).\n",
    "\n",
    "This problem is structured as follows:\n",
    "\n",
    "* Load necessary packages\n",
    "* Test the visualization of the game, to make sure everything's working\n",
    "* Process the images to reduce the dimension\n",
    "* Setup the game history buffer \n",
    "* Implement the core Q-learning function\n",
    "* Run the learning algorithm\n",
    "* Interpret the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Flappy Bird game is requires a few Python packages. Please install these _as soon as possible_, and notify us of any issues you experience so that we can help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pygame\n",
    "%pip install opencv-python\n",
    "import numpy as np\n",
    "import cv2\n",
    "import wrapped_flappy_bird as flappy_bird\n",
    "from collections import deque\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Flappy Bird environment \n",
    "\n",
    "Interaction with the game environment is carried out through calls of the form\n",
    "\n",
    "`(image, reward, terminal) = game.frame_step(action)`\n",
    "\n",
    "where the meaning of these variables is as follows:\n",
    "\n",
    "- `action`: $\\binom{1}{0}$ for doing nothing, $\\binom{0}{1}$ for \"flapping the bird's wings\"\n",
    "- `image`: the image for the next step of the game, of size $(288, 512, 3)$ with three RGB channels\n",
    "- `reward`: the reward received for taking the action; -1 if an obstacle is hit, 0.1 otherwise. \n",
    "- `terminal`: `True` if an obstacle is hit, otherwise `False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the game interface.\n",
    "First, initiate the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 2\n",
    "\n",
    "# initiate a game\n",
    "game = flappy_bird.GameState()\n",
    "\n",
    "# get the first state by doing nothing\n",
    "do_nothing = np.zeros(num_actions)\n",
    "do_nothing[0] = 1\n",
    "image, reward, terminal = game.frame_step(do_nothing)\n",
    "\n",
    "print('shape of image:', image.shape)\n",
    "print('reward: ', reward)\n",
    "print('terminal: ', terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above cells, a window should pop up, and you can watch the game being played in that window. \n",
    "\n",
    "Let's take some random actions and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(587):\n",
    "    \n",
    "    # choose a random action \n",
    "    action = np.random.choice(num_actions)\n",
    "    \n",
    "    # create the corresponding one-hot vector\n",
    "    action_vec = np.zeros(num_actions)\n",
    "    action_vec[action] = 1\n",
    "\n",
    "    # take the action and observe the reward and the next state\n",
    "    image, reward, terminal = game.frame_step(action_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you able to see Flappy moving across the window and crashing into things? Great! If you're \n",
    "having any issues, post to EdD and we'll do our best to help you out.\n",
    "\n",
    "Here is how we can visualize a frame of the game as an image within a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image.transpose([1, 0, 2]))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the images \n",
    "\n",
    "Alright, next we need to prepocess the images by converting them to grayscale and resizing them to $80\\times 80$ pixels. This will help \n",
    "to reduce the computation, and aid learning. Besides, Flappy is \n",
    "\"color blind.\" (Fun fact: The instructor of this course is also \n",
    "[color vision deficient](https://en.wikipedia.org/wiki/Color_blindness).)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_gray(frame):\n",
    "    frame = cv2.cvtColor(cv2.resize(frame, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, frame = cv2.threshold(frame, 1, 255, cv2.THRESH_BINARY)\n",
    "    return np.reshape(frame, (80, 80, 1))\n",
    "\n",
    "image_transformed = resize_gray(image)\n",
    "print('Shape of the transformed image:', image.shape)\n",
    "\n",
    "# show the transformed image\n",
    "_ = plt.imshow(image_transformed.transpose((1, 0, 2)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the preprocessed image for a single frame of\n",
    "the game. In our implementation of Deep Q-Learning, we encode the state by stacking four consecutive frames, resulting in \n",
    "a tensor of shape (80,80,4). \n",
    "\n",
    "Then, given the `current_state`, and a raw image `image_raw`\n",
    "of size $288\\times512\\times3$, we convert \n",
    "the raw image to a $80\\times80\\times 1$ grayscale image using the\n",
    "code in the previous cell. The , \n",
    "we remove the first frame of `current_state` and add \n",
    "the new frame, giving again a stack of images of \n",
    "size (80, 80, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image_raw, current_state=None):\n",
    "    # resize and convert to grayscale\n",
    "    image = resize_gray(image_raw)\n",
    "    # stack the frames\n",
    "    if current_state is None:\n",
    "        state = np.concatenate((image, image, image, image), axis=2)\n",
    "    else:\n",
    "        state = np.concatenate((image, current_state[:, :, :3]), axis=2)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Explain the game state\n",
    "\n",
    "Why is the state chosen to be a stack of four consecutive\n",
    "frames rather than a single frame? Give an intuitive explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answer here in Markdown]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Constructing the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to construct the neural network for approximating the Q function. Recall that, given input $s$ which is of size $80\\times80\\times4$ due to the \n",
    "previous preprocessing, the output of the network should be of size 2, corresponding to the values of $Q(s,a_1)$ and $Q(s, a_2)$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the summary of the model we'd like to build:\n",
    "\n",
    "![Neural network](https://raw.githubusercontent.com/YData123/sds365-sp22/main/assignments/assn4/images/q_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Initialize the network\n",
    "\n",
    "Complete the code in the next cell so that your model architecture matches that in the above picture. Here we specify the initialization of the weights by using `keras.initializers`.\n",
    "Note that we haven't talked about the `strides` argument for CNNs; \n",
    "you can read about stride here: [https://machinelearningmastery.com/padding-and-stride-for-convolutional-neural-networks/](https://machinelearningmastery.com/padding-and-stride-for-convolutional-neural-networks/). It's not important to understand this in detail, you just need to choose the number and sizes of the filters to get the shapes to match the specification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import initializers\n",
    "def create_q_model():\n",
    "    state = layers.Input(shape=(80, 80, 4,))\n",
    "\n",
    "    layer1 = layers.Conv2D(filters=..., kernel_size=..., strides=4, activation=\"relu\",\n",
    "                           kernel_initializer=initializers.TruncatedNormal(mean=0., stddev=0.01),\n",
    "                           bias_initializer=initializers.Constant(0.01))(state)\n",
    "    layer2 = layers.MaxPool2D(..., strides=2, padding=\"SAME\")(layer1)\n",
    "    layer3 = layers.Conv2D(filters=..., kernel_size=..., strides=2, activation=\"relu\", \n",
    "                           kernel_initializer=initializers.TruncatedNormal(mean=0., stddev=0.01),\n",
    "                           bias_initializer=initializers.Constant(0.01))(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    q_value = layers.Dense(units=..., activation=\"linear\", \n",
    "                           kernel_initializer=initializers.TruncatedNormal(mean=0., stddev=0.01),\n",
    "                           bias_initializer=initializers.Constant(0.01))(layer4)\n",
    "\n",
    "    return keras.Model(inputs=state, outputs=q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model summary to make sure that the network is the same as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_q_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-learning\n",
    "\n",
    "We're now ready to implement the Q-learning algorithm.\n",
    "There are some subtle details in the implementation that you need to sort out. First, recall that the update rule for Q learning is\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r(s,a) + \\gamma\\cdot \\max_{a'} Q(\\text{next}(s,a), a') - Q(s,a))$$\n",
    "\n",
    "where $\\gamma$ is the discount factor and $\\alpha$ can be viewed as the step size or learning rate for gradient ascent.\n",
    "\n",
    "We'll set these as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99            # decay rate of past observations\n",
    "step_size = 1e-4        # step size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation with experience replay\n",
    "\n",
    "At the beginning of training, we spend 10,000 steps taking random \n",
    "actions, as a means of observing the environment. \n",
    "\n",
    "We build a replay memory of length 10,000 steps, and every time we update the weights of the network, we sample a batch of size 32 and perform a Q-learning update on this batch.\n",
    "\n",
    "After we have collected 10,000 steps of new data, we discard \n",
    "the old data, and replace it with the new \"experiences.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observe = 10000            # timesteps to observe before training\n",
    "replay_memory = 10000      # number of previous transitions to remember\n",
    "batch_size = 32            # size of each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3 Justify the data collection\n",
    "\n",
    "Why does it make sense to maintain the replay memory of a fixed size \n",
    "instead of including all of the historical data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer here in Markdown]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration vs exploitation\n",
    "\n",
    "When performing Q-learning, we face the tradeoff between exploration and \n",
    "exploitation.  To encourage exploration, a simple strategy is to take a random action at each step with certain probability.\n",
    "\n",
    "More precisely, for each time step $t$ and state $s_t$, with probability $\\epsilon$, the algorithm takes a random action (wing flap or do nothing), and with probability $1-\\epsilon$ the \n",
    "algorithm takes a greedy action according to $a_t = \\arg\\max_{a} Q_\\theta(s_t,a)$. Here $\\theta$ refers to the parameters of our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value of epsilon\n",
    "epsilon = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Complete the Q-learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will need to complete the Q-learning algorithm by filling in the missing code in the following function.\n",
    "The missing parts include\n",
    "\n",
    "- Taking a greedy action\n",
    "- Given a batch of samples $\\{(s_t, a_t, r_t, s_{t+1}, \\text{terminal}_t)\\}_{t\\in B}$, computing the corresponding $Q_\\theta(s_t, a_t)$.\n",
    "- Given a batch of samples $\\{(s_t, a_t, r_t, s_{t+1}, \\text{terminal}_t)\\}_{t\\in B}$, computing the corresponding updated Q-values \n",
    "  \n",
    "$$\\hat{y}(s_t,a_t) = \\begin{cases}\n",
    "r_t + \\gamma\\, \\max_{a} Q_\\theta(s_{t+1}, a), & \\text{if } \\text{terminal}_t=0,\\\\\n",
    "r_t, & \\text{otherwise}.\n",
    "\\end{cases}$$\n",
    "\n",
    "Then, the mean squared error loss for the batch is\n",
    "\n",
    "$$\\frac{1}{|B|} \\sum_{t\\in B} (\\hat y(s_t, a_t) - Q_\\theta(s_t, a_t))^2.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dql_flappy_bird(model, optimizer, loss_function):\n",
    "\n",
    "    # initiate a game\n",
    "    game = flappy_bird.GameState()\n",
    "\n",
    "    # store the previous state, action and transitions\n",
    "    history_data = deque()\n",
    "\n",
    "    # get the first observation by doing nothing and preprocess the image\n",
    "    do_nothing = np.zeros(num_actions)\n",
    "    do_nothing[0] = 1\n",
    "    image, reward, terminal = game.frame_step(do_nothing)\n",
    "\n",
    "    # preprocess to get the state\n",
    "    current_state = preprocess(image_raw=image)\n",
    "    \n",
    "    # training\n",
    "    t = 0\n",
    "\n",
    "    while t < 50000:\n",
    "        if epsilon > np.random.rand(1)[0]:\n",
    "            # random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # compute the Q function\n",
    "            current_state_tensor = tf.convert_to_tensor(current_state)\n",
    "            current_state_tensor = tf.expand_dims(current_state_tensor, 0)\n",
    "            q_value = model(current_state_tensor, training=False)\n",
    "            \n",
    "            # greedy action\n",
    "            #-----MISSING-----# \n",
    "            # your code here\n",
    "            #-----------------#\n",
    "\n",
    "        # take the action and observe the reward and the next state\n",
    "        action_vec = np.zeros([num_actions])\n",
    "        action_vec[action] = 1\n",
    "        image_raw, reward, terminal = game.frame_step(action_vec)\n",
    "        next_state = preprocess(current_state=current_state, \n",
    "                                image_raw=image_raw)\n",
    "        \n",
    "        # store the observation\n",
    "        history_data.append((current_state, action, reward, next_state, \n",
    "                            terminal))\n",
    "        if len(history_data) > replay_memory:\n",
    "            history_data.popleft()  # discard old data\n",
    "\n",
    "        # train if done observing\n",
    "        if t > observe:\n",
    "\n",
    "            # sample a batch\n",
    "            batch = random.sample(history_data, batch_size)\n",
    "            state_sample = np.array([d[0] for d in batch])\n",
    "            action_sample = np.array([d[1] for d in batch])\n",
    "            reward_sample = np.array([d[2] for d in batch])\n",
    "            state_next_sample = np.array([d[3] for d in batch])\n",
    "            terminal_sample = np.array([d[4] for d in batch])\n",
    "\n",
    "            # compute the updated Q-values for the samples\n",
    "            #-----MISSING-----#\n",
    "            # your code here\n",
    "            #-----------------#\n",
    "\n",
    "            # train the model on the states and updated Q-values\n",
    "            with tf.GradientTape() as tape:\n",
    "                # compute the current Q-values for the samples\n",
    "                #-----MISSING-----#\n",
    "                # your code here\n",
    "                #-----------------#\n",
    "\n",
    "                # compute the loss\n",
    "                loss = loss_function(updated_q_value, current_q_value)\n",
    "\n",
    "            # backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        # update current state and counter\n",
    "        current_state = next_state\n",
    "        t += 1\n",
    "\n",
    "        # print info every 500 steps\n",
    "        if t % 500 == 0:\n",
    "            print(f\"STEP {t} | PHASE {'observe' if t<=observe else 'train'}\", \n",
    "                  f\"| ACTION {action} | REWARD {reward} | LOSS {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're now ready to play the game! Just run the cell below; do not change the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playgame(start_from_ckpt=False, ckpt_path=None):\n",
    "\n",
    "    #! DO NOT change the random seed !\n",
    "    np.random.seed(4)\n",
    "\n",
    "    if start_from_ckpt:\n",
    "        # if you want to start from a checkpoint\n",
    "        model = keras.models.load_model('ckpt_path')\n",
    "    else:\n",
    "        model = create_q_model()\n",
    "\n",
    "    # specify the optimizer and loss function\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=step_size, clipnorm=1.0)\n",
    "    loss_function = keras.losses.MeanSquaredError()\n",
    "\n",
    "    # play the game\n",
    "    dql_flappy_bird(model=model, optimizer=optimizer, loss_function=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playgame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Describe the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe what you see by answering the following questions:\n",
    "\n",
    "- In the early stage of training (within 2,000 steps in the *explore* phase), \n",
    "  describe the behavior of the Flappy Bird. What do you think is the greedy policy \n",
    "  given by the estimation of the Q-function in this stage?\n",
    "- Describe what you see after roughly 5,000 training steps. \n",
    "  Do you see any improvement?\n",
    "  In particular, compare Flappy's behavior with their behavior in the early stages of \n",
    "  training.\n",
    "- Explain why the performance has improved, by relating to the model \n",
    "  design such as the replay memory and the exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer here in Markdown]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes a long time to fully train the network, so you're not required to \n",
    "complete the training. Here's a [video](https://www.youtube.com/watch?v=THhUXIhjkCM) showing the performance of a well trained DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Julius Tensor (15 points)\n",
    "\n",
    "In this problem you will modify a basic vanilla \"np-complete\" implementation of recurrent neural networks, and evaluate some pre-trained RNNs trained on the plays of \n",
    "William Shakespeare.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shakespeare(model, size=1000, seed='ROMEO:'):\n",
    "    states = None\n",
    "    next_char = tf.constant([seed])\n",
    "    result = [next_char]\n",
    "                             \n",
    "    for n in range(1000):\n",
    "      next_char, states = model.generate_one_step(next_char, states=states)\n",
    "      result.append(next_char)\n",
    "\n",
    "    return tf.strings.join(result)[0].numpy().decode(\"utf-8\")\n",
    "                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.1: Word play\n",
    "\n",
    "We have upload three pre-trained models for you to experiment with: 'Hamlet', 'Macbeth', and 'Comedy of Errors'. One of the models has 256 hidden neurons in the GRU layer, and was trained\n",
    "for 30 epochs. The two other models each have have 1024 hidden neurons in the GRU layer; one was trained for 30 epochs, and the other trained for 50 epochs. \n",
    "\n",
    "Generate sample text from each, using the function `generate_shakespeare` defined above, in order to evaluate the models. Which model is which? Explain how you made your assessment.\n",
    "\n",
    "You can download the models 'Hamlet', 'Comedy_of_Errors' and 'Macbeth' from \n",
    "https://github.com/YData123/sds365-sp22/tree/main/assignments/assn4.\n",
    "\n",
    "Note: If you are unable to load and run these models on your own computer, please run \n",
    "them in the cloud on Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained1 = tf.saved_model.load('Hamlet')\n",
    "pretrained2 = tf.saved_model.load('Comedy_of_Errors')\n",
    "pretrained3 = tf.saved_model.load('Macbeth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code and markdown here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.2: np-complete implementation of RNNs\n",
    "\n",
    "Run the [basic rnn code](https://github.com/YData123/sds365-sp22/blob/main/assignments/assn4/shakespeare_rnn.py) from [Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) to train a model on the [Shakespeare data](https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt). \n",
    "Modify the code however you see fit, for example, by changing how often a snippet of synthetic text is generated.\n",
    "\n",
    "* How many steps of gradient descent did you train for? How many characters of Shakespeare data was processed during training? How long did the training take?\n",
    "\n",
    "* Did you modify the code in any way? If so, how?\n",
    "\n",
    "* Provide a few examples of generated text from your trained model.\n",
    "\n",
    "* How do the generated samples compare to those generated\n",
    "in Problem 4.1?\n",
    "\n",
    "* Describe at least three differences between this vanilla RNN model\n",
    "and the GRU networks that you experimented with in 4.1, which are described [here](https://www.tensorflow.org/text/tutorials/text_generation).\n",
    "\n",
    "Note: To limit the output the graders need to examine, please experiment in a separate Python session, and only include your final results in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code and markdown here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.3: Adding a layer \n",
    "\n",
    "Now suppose you wish to add another recurrent hidden layer to the np-complete RNN implementation. Rather than implementing the full model training, \n",
    "we'd like you to implement the generator. (But we encourage you to at least partially implement the full training of the two-layer model.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the generator for the model that has a single recurrent layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is a seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "        \n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your job is to rewrite this function for a model that has two hidden recurrent layers.\n",
    "Complete the implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample2(h1, h2, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h1, h2 is memory state, seed_ix is a seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        ...\n",
    "\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain your implementation, by giving a description of each line of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.4 Be the Bard (optional, 2 points EC)\n",
    "\n",
    "Starting from the [TensorFlow tutorial](https://www.tensorflow.org/text/tutorials/text_generation), train the best model \n",
    "that you can, generating synthetic Shakespeare that is better than the best model\n",
    "from problem 4.1 above. \n",
    "\n",
    "* Describe how the model is indeed better than the best model in 4.1.\n",
    "* Provide details on how you trained the model, and any changes made to the model \n",
    "architecture.\n",
    "* So that you do not add too much stuff to your notebook, put your code in a separate notebook (submitted to Canvas), and only add Markdown below to detail your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your markdown here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
